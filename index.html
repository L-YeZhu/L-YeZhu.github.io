<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Ye Zhu | Home</title>
  <meta name="description" content="Ye Zhu">
  <meta name="author" content="Ye Zhu">
  <meta property="og:title" content="Ye Zhu" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="http://" />
  <meta property="og:site_name" content="Ye Zhu" />
  <link rel="canonical" href="http://" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=/libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=/libs/custom/my_css.css>

  <!-- JQuery
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src=/libs/external/jquery-3.1.1.min.js></script>

  <!-- Font-Awesome
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/font-awesome-4.7.0/css/font-awesome.min.css>

  <!-- Academicons
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/academicons-1.8.6/css/academicons.min.css>

  <!-- Skeleton tabs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=/libs/external/skeleton_tabs/skeleton-tabs.js></script>

  <!-- Timeline
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/timeline.css>

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!--<link rel="stylesheet" href=/libs/external/github-prettify-theme.css>-->
  <script src=/libs/custom/my_js.js></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">

    <section class="header">
      <div class="row">
        <div class="three columns">
          <a href="/"><img class="u-max-full-width" src='/assets/profile-pics/profile.jpg'></a>
        </div>
        <div class="nine columns main-description">
            <h1>Ye Zhu</h1>
            <p>Postdoc Researcher in Computer Science <br> Princeton University, NJ, USA</p>
            <p>Contact: yezhu [AT] princeton.edu</p>
            <!-- <p>https://l-yezhu.github.io/assets/cv/cv.pdf</p> -->
            <p>


<!--               <span onclick="window.open('')" style="cursor: pointer">
                <i class="fa fa-linkedin-square" aria-hidden="true"></i>
              </span> -->
              
              <span onclick="window.open('https://github.com/L-YeZhu')" style="cursor: pointer">
                <i class="fa fa-github" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://scholar.google.com/citations?user=uk5WuyIAAAAJ&hl=en&authuser=1')" style="cursor: pointer">
                <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://twitter.com/szyezhu')" style="cursor: pointer">
                <i class="fa fa-twitter" aria-hidden="true"></i>
              </span>
            </p>
        </div>
      </div>
    </section>

    <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href=/index.html#bio>About Me</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#news>News</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#publications>Main Research</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#publications2>ML4Astro</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#>Talks</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#>Tidbits</a></li>
        </ul>
      </div>
    </nav>

    <!-- ========== BIO ========== -->
<div class="docs-section" id="bio">
  <h4>About Me</h4>


  <p>
I am a postdoc research associate in Computer Science at Princeton University, working with <a href="https://www.cs.princeton.edu/~olgarus/">Prof. Olga Russakovsky</a> at the VisualAI lab. My main research works focus on multimodal learning and generation (vision, audio, and language), with interdisciplinary research interests in Machine Learning for Astrophysics. 
</p>

<p>
Prior to the postdoc, I received my Ph.D. in Computer Science from Illinois Institute of Technology, USA in 2023; and my M.S. and B.S. in Mechanical Engineering from Shanghai Jiao Tong University, China, in 2019 and 2016, respectively. I received the French Engineering Education system and studied at Ecole Polytechnique in France in 2016 and 2017. I was a research intern at Bang&Olufsen in Denmark in 2018, and at Snap in the USA in 2021.
</p>

<p>
I serve as a reviewer for CVPR/ECCV/ICCV/NeurIPS/ICLR/ICML/AAAI. 
I also hold French language diplomas in both DALF and TCF C1, and was a part-time French translator for the European scientific magazine Science & Vie during 2015 - 2022 (sadly stopped due to a heavy research workload).
</p>

<p>
Find full <a href=/assets/cv/cv.pdf target="_blank">CV/resume</a> with last update in 09/2023.
</p>


</div>


<!-- ========== BIO ========== -->
<div class="docs-section" id="news">
  <h4>News</h4>
  <p>
    09/2023: Our BoundaryDiffusion paper accepted to NeurIPS 2023.
  </p>

  <p>
    09/2023: Finish my Ph.D. defense and join Princeton CS as a postdoc researcher.
  </p>
  <p>
    04/2023: Our first #ML4Astro work that applies Diffusion Models to predict the density of molecular clouds has been accepted to the top astronomy venue The Astrophysical Journal (APJ). Chere <a href="https://arxiv.org/abs/2304.01670">here</a>!
  </p>
  <p>
  01/2023: One paper about Discrete Contrastive Diffusion accepted to ICLR2023.
  </p>
  <p>
  01/2023: Received the Award of Excellence in Dissertation Research for the College of Computing at IIT.
  </p>
  <p>
  07/2022: One paper about dance-to-music cross modality generation accepted to ECCV2022.
  </p>
<!--   <p>
  06/2022: Great pleasure to join the <a href="https://visualai.princeton.edu/" target="_blank">Princeton Visual AI lab</a> as a visiting researcher, working with <a href="https://www.cs.princeton.edu/~olgarus/" target="_blank">Prof. Olga Russakovsky</a>. 
  </p>
  <p>
  05/2022: Recieved the CVPR 2022 travel grant.
  </p>
  <p>
  07/2021: One paper about vision and language accepted to TPAMI 2021.
  </p>
  <p>
  05/2021: Start research internship at Snap Inc..
  </p> -->



</div>



<!-- ========== PUBLICATIONS ========== -->
<div class="docs-section" id="publications">
  <h4>Featured Publications</h4>

  <p>Complete publications on <a href="https://scholar.google.com/citations?user=uk5WuyIAAAAJ&hl=en&authuser=1" target="_blank">Google Scholar</a>.<br/>
  </p>



  <div class="tab-content">
    <div class="tab-pane active" id="papers-selected">
      
      
        <div class="paper">
          <p class="title"><b>Boundary Guided Mixing Trajectory for Semantic Control with Diffusion Models</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Zhiwei Deng, Olga Russakovsky, and Yan Yan</p>
          <p><i> Conference on Neural Information Processing Systems<b>(NeurIPS)</b>, 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2302.08357" target="_blank">Paper</a> 
              <a class="button" href="https://github.com/L-YeZhu/BoundaryDiffusion" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Vision+X: A Survey on Multimodal Learning in the Light of Data</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Nicu Sebe, and Yan Yan</p>
          <p><i>arXiv preprint</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2210.02884" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Discrete Contrastive Diffusion for Cross-Modal Music and Image Generation</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, and Yan Yan</p>
          <p><i>International Conference on Learning Representations <b>(ICLR)</b>, 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2206.07771" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/CDCD" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Quantized GAN for Complex Music Generation from Dance Videos</b></p>
          <p><u><b>Ye Zhu</b></u>, Kyle Olszewski, Yu Wu, Panos Achlioptas, Menglei Chai, Yan Yan, and Sergey Tulyakov</p>
          <p><i>European Conference on Computer Vision <b>(ECCV)</b>, 2022</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2204.00604" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/D2M-GAN" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Saying the Unseen: Video Descriptions via Dialog Agents</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Yi Yang, and Yan Yan</p>
          <p><i>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2106.14069.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Video-Description-via-Dialog-Agents-ECCV2020" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning Audio-Visual Correlations From Variational Cross-Modal Generations</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Hugo Latapie, Yi Yang, and Yan Yan</p>
          <p><i>IEEE International Conference on Acoustics, Speech and Signal Processing <b>(ICASSP)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2102.03424.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Learning-Audio-Visual-Correlations" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion Network for Action Recognition</b></p>
          <p>Xiaoguang Zhu, <u><b>Ye Zhu</b></u>, Haoyu Wang, Honglin Wen, Yan Yan, Peilin Liu.</p>
          <p><i>Transactions on Multimedia Computing Communications and Applications <b>(TOMM)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2202.11374" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Yi Yang, and Yan Yan</p>
          <p><i>European Conference on Computer Vision <b>(ECCV)</b>, 2020</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2008.07935.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Video-Description-via-Dialog-Agents-ECCV2020" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Hierarchical HMM for Eye Movement Classifications</b></p>
          <p><u><b>Ye Zhu</b></u>, Yan Yan, and  Oleg Komogortsev</p>
          <p><i>European Conference on Computer Vision Workshop <b>(ECCV Workshop)</b>, 2020</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2008.07961.pdf" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
    </div>

    <div class="tab-pane" id="papers-all">
      
        <div class="paper">
          <p class="title"><b>Boundary Guided Mixing Trajectory for Semantic Control with Diffusion Models</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Zhiwei Deng, Olga Russakovsky, and Yan Yan</p>
          <p><i> Conference on Neural Information Processing Systems<b>(NeurIPS)</b>, 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2302.08357" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/BoundaryDiffusion" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Denoising Diffusion Probabilistic Models to Predict the Number Density of Molecular Clouds in Astronomy</b></p>
          <p>Duo Xu, Jonathan Tan*, Chia-Jung Hsu, and <u><b>Ye Zhu</b></u></p>
          <p><i>International Conference on Learning Representations Physics4ML Workshop <b>(ICLR Workshop)</b>, 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/forum?id=KiwRgaRYqRE" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Vision+X: A Survey on Multimodal Learning in the Light of Data</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Nicu Sebe, and Yan Yan</p>
          <p><i>arXiv preprint</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2210.02884" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Discrete Contrastive Diffusion for Cross-Modal Music and Image Generation</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, and Yan Yan</p>
          <p><i>International Conference on Learning Representations <b>(ICLR)</b>, 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2206.07771" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/CDCD" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Quantized GAN for Complex Music Generation from Dance Videos</b></p>
          <p><u><b>Ye Zhu</b></u>, Kyle Olszewski, Yu Wu, Panos Achlioptas, Menglei Chai, Yan Yan, and Sergey Tulyakov</p>
          <p><i>European Conference on Computer Vision <b>(ECCV)</b>, 2022</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2204.00604" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/D2M-GAN" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Supplementing Missing Visions via Dialog for Scene Graph Generation</b></p>
          <p><u><b>Ye Zhu</b></u>, Xiaoguang Zhu, Yuzhang Shang, Zhenhao Zhao, and Yan Yan</p>
          <p><i>arXiv preprint</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2204.11143.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/SI-Dial" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Saying the Unseen: Video Descriptions via Dialog Agents</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Yi Yang, and Yan Yan</p>
          <p><i>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2106.14069.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Video-Description-via-Dialog-Agents-ECCV2020" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning Audio-Visual Correlations From Variational Cross-Modal Generations</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Hugo Latapie, Yi Yang, and Yan Yan</p>
          <p><i>IEEE International Conference on Acoustics, Speech and Signal Processing <b>(ICASSP)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2102.03424.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Learning-Audio-Visual-Correlations" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion Network for Action Recognition</b></p>
          <p>Xiaoguang Zhu, <u><b>Ye Zhu</b></u>, Haoyu Wang, Honglin Wen, Yan Yan, Peilin Liu.</p>
          <p><i>Transactions on Multimedia Computing Communications and Applications <b>(TOMM)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2202.11374" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Yi Yang, and Yan Yan</p>
          <p><i>European Conference on Computer Vision <b>(ECCV)</b>, 2020</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2008.07935.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Video-Description-via-Dialog-Agents-ECCV2020" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Hierarchical HMM for Eye Movement Classifications</b></p>
          <p><u><b>Ye Zhu</b></u>, Yan Yan, and  Oleg Komogortsev</p>
          <p><i>European Conference on Computer Vision Workshop <b>(ECCV Workshop)</b>, 2020</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2008.07961.pdf" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
    </div>
  </div>
</div>


<div class="docs-section" id="publications2">
  <h4>ML4Astrophysics</h4>

  <p>Complete publications on <a href="https://scholar.google.com/citations?user=uk5WuyIAAAAJ&hl=en&authuser=1" target="_blank">Google Scholar</a>.<br/>
  </p>



  <div class="tab-content">
    <div class="tab-pane active" id="papers-selected">
      
      
        <div class="paper">
          <p class="title"><b>Denoising Diffusion Probabilistic Models to Predict the Density of Molecular Clouds</b></p>
          <p>Duo Xu, Jonathan C. Tan, Chia-Jung Hsu, and <u><b>Ye Zhu</b></u></p>
          <p><i>The Astrophysical Journal (APJ), 2023 & ICLR Physics4ML Workshop 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2302.08357" target="_blank">Main Paper</a>
            
            
              <a class="button" href="https://openreview.net/forum?id=KiwRgaRYqRE" target="_blank">Workshop</a>
            
            

            

            

            
          </div>
        </div>
      
    </div>

    
  </div>
</div>


<div class="docs-section" id="talk">
  <h4>Selected Talks and Travel</h4>

  <p>
    Upcoming:
  </p>

  <p>
    12/2023: NeurIPS2023, New Orleans, USA.
  </p>

  <p>
    10/2023: Bang&Olufsen, Copenhagen, Denmark.
  </p>

  <p>
    10/2023: ICCV2023, Paris, France.
  </p>

   <p>
  05/2023: Talk on Diffusion Models at Shanghai Jiao Tong University, Shanghai, China.
  </p> 

  <p>
    05/2023: Talk on Diffusion Models at Wuhan University, Wuhan, China.
  </p>

    <p>
    05/2023: ICLR2023, Kigali, Rwanda.
  </p>


   <p>
  04/2023: Guest lecture on Diffusion Models for COS429 at Princeton University, USA.
  </p> 

   <p>
  04/2023: Talk on Diffusion Models at PIXL Lunch, Princeton University, USA.
  </p> 


<!--    <p>
  11/2022: Talk (remote) on Multimodal Generation for Audio and Music at Bang & Olufsen, Denmark.
  </p>  -->


</div>


<!-- ========== RESUME ========== -->
<div class="docs-section" id="tidbit">
  <h4>Tidbits</h4>

     <p>
 My full name is 竺 烨, which shares the exact pronunciation as "Bamboo Leaf" in Chinese.
  </p> 

<p>
I enjoy reading, hiking, cooking, traveling, and taking random photos. I read books about Chinese traditional culture and history daily, such as 庄子 (Zhuangzi) and 周易 (Zhouyi), and find them especially inspiring to research works ;).
</p>




    <div class="footer">
      <div class="row">
        <div class="four columns">
          Ye Zhu
        </div>
        <div class="four columns">
          Contact: yezhu [AT] princeton.edu
        </div>
        <div class="four columns">
<!--           <span onclick="window.open('')" style="cursor: pointer">
            <i class="fa fa-linkedin-square" aria-hidden="true"></i>
          </span> -->
          <span onclick="window.open('https://github.com/L-YeZhu')" style="cursor: pointer">
            <i class="fa fa-github" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://scholar.google.com/citations?user=uk5WuyIAAAAJ&hl=en&authuser=1')" style="cursor: pointer">
            <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://twitter.com/szyezhu')" style="cursor: pointer">
            <i class="fa fa-twitter" aria-hidden="true"></i>
          </span>
        </div>
      </div>
    </div>

  </div>

  <!-- Google Analytics -->
 <!--  <script>
  // (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  // (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  // m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  // })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  // ga('create', '', 'auto');
  // ga('send', 'pageview');

</script> -->

  <!-- do not remove -->
  <span id="62cd7b7da1aff3196fdc26b60e396df9"></span>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
